{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation & Setup"
      ],
      "metadata": {
        "id": "bvF2LUKURqhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install nltk\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install srt\n",
        "!pip install gdown\n",
        "!apt install ffmpeg\n",
        "!pip install deepmultilingualpunctuation\n",
        "!pip install silero-vad\n",
        "!pip install spacy\n",
        "!pip install pytextrank\n",
        "!pip install pydub\n",
        "\n",
        "!pip install git+https://github.com/m-bain/whisperX.git"
      ],
      "metadata": {
        "id": "MaAi3yOUWxq0"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "import gdown\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import spacy\n",
        "import srt\n",
        "from functools import reduce\n",
        "from pydub import AudioSegment\n",
        "\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn.functional as F\n",
        "from transformers import \\\n",
        "LongformerTokenizer, LongformerModel, LongformerForSequenceClassification, \\\n",
        "RobertaTokenizer, RobertaForTokenClassification, Trainer, TrainingArguments, \\\n",
        "LEDTokenizer, LEDForConditionalGeneration\n",
        "\n",
        "# Text\n",
        "import pytextrank\n",
        "\n",
        "# Audio\n",
        "import whisperx\n",
        "import silero_vad\n",
        "from silero_vad import load_silero_vad, read_audio, get_speech_timestamps\n",
        "\n",
        "# Video"
      ],
      "metadata": {
        "id": "oiFdk6oSYg8w"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "uezzF1YfwZ8S",
        "outputId": "6fd3395c-241c-438b-c955-07fb078c6f5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook config\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "oHj-ew8rbyYB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variables"
      ],
      "metadata": {
        "id": "LYkbiBtust3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_input = \"dataset/teamwork in the classroom.mov\"\n",
        "\n",
        "full_base = os.path.dirname(video_input)\n",
        "path_dataset = full_base\n",
        "filename = os.path.basename(video_input)\n",
        "filename_without_extension = os.path.splitext(filename)[0]\n",
        "\n",
        "filename_video_input = filename\n",
        "filename_subtitles_output = filename_without_extension + \".srt\"\n",
        "filename_audio_output = filename_without_extension + \".wav\"\n",
        "\n",
        "subtitles_output = os.path.join(full_base, filename_subtitles_output)\n",
        "audio_output = os.path.join(full_base, filename_audio_output)\n",
        "\n",
        "video = ''\n",
        "audio = ''\n",
        "subtitles = ''\n",
        "sentences = ''"
      ],
      "metadata": {
        "id": "5qOwE-2vstJc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(audio_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx1C6Lk35JbV",
        "outputId": "1c16ac64-b1a6-448b-bbe9-f2357c19162b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset/teamwork in the classroom.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "0E33VpXsRdq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive Dataset Location\n",
        "folder_id = '1k7DLJPl1xz9lpU4l3dZYtPe1XawhrXeC' # taken from drive.google.com/drive/u/1/folders/1k7D...(this part)\n",
        "gdown.download_folder(id=folder_id, quiet=False, use_cookies=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3N4G0ODPvfr",
        "outputId": "34241153-ce80-4c1a-a7f0-16064056da14"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1vuIW3CVm2p_Ig-_srJ5sIwUxzTGqdGHp assessing students without exams.mov\n",
            "Processing file 1OP3zzSmpKJ0RDPasl9AGQD2yNeCEXtoR flipped learning basics.mov\n",
            "Processing file 1wslcvTNd88FQMXJgvGbKR3sjwXORR6xt teamwork in the classroom.mov\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1vuIW3CVm2p_Ig-_srJ5sIwUxzTGqdGHp\n",
            "From (redirected): https://drive.google.com/uc?id=1vuIW3CVm2p_Ig-_srJ5sIwUxzTGqdGHp&confirm=t&uuid=19df07d6-2d86-4e16-a7fe-66bf3650c43d\n",
            "To: /content/dataset/assessing students without exams.mov\n",
            "100%|██████████| 875M/875M [00:11<00:00, 78.2MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1OP3zzSmpKJ0RDPasl9AGQD2yNeCEXtoR\n",
            "From (redirected): https://drive.google.com/uc?id=1OP3zzSmpKJ0RDPasl9AGQD2yNeCEXtoR&confirm=t&uuid=956ab9f8-6d62-4814-b5f8-fd72055b1f65\n",
            "To: /content/dataset/flipped learning basics.mov\n",
            "100%|██████████| 399M/399M [00:12<00:00, 32.0MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1wslcvTNd88FQMXJgvGbKR3sjwXORR6xt\n",
            "From (redirected): https://drive.google.com/uc?id=1wslcvTNd88FQMXJgvGbKR3sjwXORR6xt&confirm=t&uuid=c160edb0-a1de-4b1d-b36b-d7823cf0a38a\n",
            "To: /content/dataset/teamwork in the classroom.mov\n",
            "100%|██████████| 202M/202M [00:02<00:00, 80.3MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/dataset/assessing students without exams.mov',\n",
              " '/content/dataset/flipped learning basics.mov',\n",
              " '/content/dataset/teamwork in the classroom.mov']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Test Dataset:\n",
        "paragrah_simple = \"Renewable energy is crucial for reducing carbon emissions. Solar power, in particular, is sustainable and abundant. Interestingly, solar panels were first invented in 1954. With continued advancements, solar energy is becoming more accessible in everyday life.\"\n",
        "paragraph_simple_unpunct = \"Renewable energy is crucial for reducing carbon emissions  Solar power, in particular, is sustainable and abundant Interestingly, solar panels were first invented in 1954 With continued advancements, solar energy is becoming more accessible in everyday life\"\n",
        "\n",
        "# Other: CNN/Daily Mail\n",
        "# dataset_news = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "# paragraph_news = dataset_news['train']['article'][0]\n",
        "# summary_news = dataset_news['train']['highlights'][0]"
      ],
      "metadata": {
        "id": "KIZL57wtPiYb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SRT  \n",
        "each **`subtitle`** in the subtitles array has the following properties:\n",
        "\n",
        "1. **`index`**\n",
        "   - The sequential number of the subtitle within the SRT file.\n",
        "   - `1`, `2`, `3`, etc. (Integer)\n",
        "2. **`start`**\n",
        "   - The time (in milliseconds) when the subtitle should appear on the screen.\n",
        "   - `00:00:05,000` (String representing HH:MM:SS,SSS)\n",
        "3. **`end`**\n",
        "   - The time (in milliseconds) when the subtitle should disappear from the screen.\n",
        "   - `00:00:10,000` (String representing HH:MM:SS,SSS)\n",
        "4. **`content`**\n",
        "   - The actual text of the subtitle that will be displayed.\n",
        "   - \"Hello, world!\" (String)\n",
        "5. **`proprietary`**\n",
        "   - This field holds any additional data or formatting specific to the SRT file or software used to create it. Often empty and can usually be ignored.\n",
        "   - `''` (Empty string, or sometimes contains specific formatting codes)"
      ],
      "metadata": {
        "id": "tCtO9Rg6J5Dz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "7dsF0AmX2pt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio - Load"
      ],
      "metadata": {
        "id": "_zukAoWW7Ia0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Extract audio (wav) from video\n",
        "!ffmpeg -y -i \"$video_input\" -vn -acodec pcm_s16le -ar 44100 -ac 2 \"$audio_output\""
      ],
      "metadata": {
        "id": "N_E3vUnK7GYc"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio - SRT Generation"
      ],
      "metadata": {
        "id": "-aWRBu9h7XkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Time Taken: ~4min"
      ],
      "metadata": {
        "id": "p9DVWtXh79zY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select device (GPU if available, otherwise CPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "language=\"en\"\n",
        "compute_type=\"int8\"\n",
        "\n",
        "# Model WhisperX\n",
        "model = whisperx.load_model(\"base\", device=device, language=language, compute_type=compute_type) # Choose \"base\" or \"large\" model\n",
        "\n",
        "# Transcribe audio\n",
        "result = model.transcribe(audio_output)\n",
        "\n",
        "# Align with forced alignment\n",
        "alignment_model, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "result = whisperx.align(result[\"segments\"], alignment_model, metadata, audio_output, device)\n",
        "\n",
        "# Generate SRT file with aligned sentences\n",
        "with open(subtitles_output, \"w\") as f:\n",
        "    for i, segment in enumerate(result[\"segments\"], 1):\n",
        "        # Get start and end times in SRT format\n",
        "        start_time = whisperx.utils.format_timestamp(segment[\"start\"])\n",
        "        end_time = whisperx.utils.format_timestamp(segment[\"end\"])\n",
        "\n",
        "        # Write SRT entry\n",
        "        f.write(f\"{i}\\n{start_time} --> {end_time}\\n{segment['text']}\\n\\n\")\n",
        "\n",
        "print(f\"SRT file generated: {subtitles_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG9mX-np7hl5",
        "outputId": "50e8b0a1-1359-468a-cfb8-f2e0208e78f1"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.5.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
            "SRT file generated: dataset/teamwork in the classroom.srt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text - Paragraph\n",
        "combination of all subtitle parts.  \n",
        "\n",
        "WhisperAI enhances transcription with basic punctuation."
      ],
      "metadata": {
        "id": "7vSt5ElqS2bK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming aligned_segments is your WhisperX alignment output\n",
        "paragraph = reduce(lambda acc, seg: acc + seg['text'].strip() + ' ', aligned_segments['segments'], '')\n",
        "\n",
        "# Print the paragraph\n",
        "print(paragraph)"
      ],
      "metadata": {
        "id": "OFMXfndjQPGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaa408ea-1d03-4b3f-c9bd-6b53565ae4fb"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, this is Lino Cordia and today I want to talk to you about a very important topic, challenging topic, teamwork in the classroom. So why is teamwork in the classroom so important? Well, for our students, it allows them to develop a bunch of new skills, right? Communication skills, leadership skills, et cetera. Also, when you're working with a team, you get different perspectives. Ideally, you are part of a team that has people with different genders, people with different age groups, people with different academic backgrounds, right? So when you're talking to them, you get all these fresh perspectives that inform your the task that you're trying to solve. Also teams will motivate you, they will support you, you will feel empowered by them ideally, right? This is like the things that should happen. And also this is how the world works, right? Pretty much everything we do, we need to do it as a team. Now here's the thing about teamwork in the classroom. Students hate it. But why do they hate it? Well, these are the things that I can identify. They don't know their teammates or they do know their teammates, but they don't like them. They're also concerned about an even workload, right? I'm going to be working all night while this guy is like doing something else that is not teamwork and things like that. They're also concerned about their grades. They're concerned because A lot of it is out of their control. It doesn't matter how hard I work, if my teammates are not doing enough work, it's going to impact my own grade. And so, yeah, all of these things are legitimate concerns. So what can we do as professors? These are the things that I do. I explain to my students why teamwork is so important. I tell them about my own life experiences. I am positive, but realistic. When I talk about teamwork, I acknowledge that it's challenging. And then once I have created Teams, it's important to assign class time for team building exercises. Now, if this team is going to work together for 10 minutes every week, then maybe you just need a short icebreaker. right so that they know each other and they can start working together. If you are going to have this team work together for the entire semester, it doesn't hurt to use one of your two hour lectures for stronger, longer, more comprehensive team building exercise. And the first time you give your students a task as a team, make it a low-stake task, right? Something that doesn't really impact their grade or if it does, it's tiny, it's minimal. And ideally, the first time they work together as a team, they do this in person. Working together in person always creates a stronger connection than if you do this virtually. Now, this is the best thing that I have learned about teamwork. I had this class and my students were working in teams. And after a few weeks, I saw that this team was working really well together. And so I approached them and say, why do you get along so well as a team? And their answer was we had dinner together. And so this is my advice to every team. Go and grab a coffee, go and get lunch together, something like that is going to help you connect. as a team you're going to see everybody as a person and you're going to know each other and yeah it's going to be better trust me. Now when I create teams if it's a short collaboration and by this I mean they're going to be working together about 10 minutes every every class Then you can randomly assign them, and especially if it's a large class, it's going to be a good strategy. Sometimes, especially after they know each other for a month or so, you can also let them choose their own teammates and just pick your team. If it's a long collaboration, sometimes self-selection works. I have a final project and I tell them you get to pick whoever you can work with anybody that you want. That works. But sometimes it's important that you choose a team based on the skills that the team has or based on their interest. You need to determine when is a good idea for self-selection or when is a good idea to think of. interest or skills when created a team depends on the goals of that exercise of that project, things like that. Now the size of the team, again, this is not what you should do. This is what I do. For sure collaborations, usually I create four member teams. And this is also because of the structure of the classrooms where I work. There's tables with four chairs, right? So make sense that we don't need to disrupt the classroom destroy things, move around furniture too much. But if it is one month or one semester project, I usually create teams of two or three members. I find that it's easy to identify what each member has done. It's easy also for them for communication purposes, and this is what has worked for me. But what are your strategies? I would love to learn. I'm not an expert in team building, right? I'm just sharing what I've done, what has worked for me, and what I have learned from my students. But I would love to hear your experiences, your strategies, your solutions to this teamwork issue. And yeah, please reach out. I would love to hear from you. Thank you so much. I am Lino Cordia. See you next time. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text - Paragraph Summarized"
      ],
      "metadata": {
        "id": "R-DIsC5NRn3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model: Longformer Encoder-Decoder\n",
        "model_name = \"allenai/led-base-16384\"\n",
        "tokenizer = LEDTokenizer.from_pretrained(model_name)\n",
        "model = LEDForConditionalGeneration.from_pretrained(model_name)\n",
        "text = paragraph\n",
        "\n",
        "# Tokenization\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", max_length=4096, truncation=True)\n",
        "\n",
        "# Summary Generation\n",
        "summary_ids = model.generate(inputs[\"input_ids\"], max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "paragraph_summarized = tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h82EUpYU1uU",
        "outputId": "330c39bd-f0f3-47ef-cfe2-f608269593fe"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Input ids are automatically padded from 1164 to 2048 to be a multiple of `config.attention_window`: 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text - Sentence Segmentation"
      ],
      "metadata": {
        "id": "JG0zDyIunCP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Segmentation\n",
        "sentences = sent_tokenize(paragraph)\n",
        "print(len(sentences))\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "72E_kMUVUN6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "c95c22ee-4131-4d1e-d9b9-027dd77fb356"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'paragraph' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-c3dda7eb6816>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Segmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'paragraph' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text"
      ],
      "metadata": {
        "id": "NqhKyoOO3voB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Metrics\n",
        "original_length = len(paragraph)\n",
        "summary_length = len(paragraph_summarized)\n",
        "\n",
        "print(f\"original length: {original_length}\")\n",
        "print(f\"summary length: {summary_length}\")\n",
        "\n",
        "compression_ratio = (original_length - summary_length) / original_length\n",
        "print(f\"auto-summary compression ratio: {compression_ratio:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83D8Px3ZZ25D",
        "outputId": "9ad792a3-ac8e-4eab-ba60-43cc05852c58"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original length: 5282\n",
            "summary length: 723\n",
            "auto-summary compression ratio: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metric 1: Simple Sentence-Paragraph Relevancy (Cosine Similarity)"
      ],
      "metadata": {
        "id": "_8gMAZOaQKNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# 1: Longformer Model\n",
        "tokenizer_lf = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "model_lf = LongformerModel.from_pretrained('allenai/longformer-base-4096')"
      ],
      "metadata": {
        "id": "kGDOM_LzxmeM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2: Tokenization\n",
        "paragraph_tokens = tokenizer_lf(paragraph_summarized, return_tensors='pt')\n",
        "sentence_tokens = [tokenizer_lf(sentence, return_tensors='pt') for sentence in sentences]"
      ],
      "metadata": {
        "id": "c_FszsOCzf02"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding Explanation  \n",
        "The [CLS] (classification) token is often used in transformer models to represent the overall meaning or summary of the input sequence. By extracting its embedding, you're essentially obtaining a representation that captures the main point or essence of the paragraph."
      ],
      "metadata": {
        "id": "bBHu-QLj6kq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Time Taken: ~3min\n",
        "~2.5min for 850MB video"
      ],
      "metadata": {
        "id": "FBhdvFmpM5wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3: Embedding\n",
        "with torch.no_grad(): # Disable gradient computation for efficiency\n",
        "    paragraph_embedding = model_lf(**paragraph_tokens).last_hidden_state[:, 0, :]  # Get the [CLS] token embedding\n",
        "    sentence_embeddings = [model_lf(**tokens).last_hidden_state[:, 0, :] for tokens in sentence_tokens]"
      ],
      "metadata": {
        "id": "7RBG1chM0B8x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "29a16978-5649-4073-e231-894e9b6e5188"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-27bcf9735fcd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Disable gradient computation for efficiency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mparagraph_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_lf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparagraph_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Get the [CLS] token embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msentence_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_lf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-27bcf9735fcd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Disable gradient computation for efficiency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mparagraph_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_lf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparagraph_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Get the [CLS] token embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msentence_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_lf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1727\u001b[0m         )\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1729\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1730\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, padding_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1307\u001b[0m                 )\n\u001b[1;32m   1308\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1310\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1247\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m   1250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mff_chunk\u001b[0;34m(self, attn_output)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4: Relevance scores\n",
        "relevance_scores = [torch.cosine_similarity(paragraph_embedding, sentence_embedding).item() for sentence_embedding in sentence_embeddings]\n",
        "\n",
        "# Normalization: min-max normalization\n",
        "min_score = min(relevance_scores)\n",
        "max_score = max(relevance_scores)\n",
        "normalized_scores = [(score - min_score) / (max_score - min_score) for score in relevance_scores]\n",
        "\n",
        "# round\n",
        "normalized_scores = [np.format_float_positional(score, precision=2, unique=False, fractional=False, trim='k') for score in normalized_scores]"
      ],
      "metadata": {
        "id": "9BwrHjKc4Tge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5: Display Results\n",
        "df = pd.DataFrame({\"Sentence Index\": range(len(sentences)), \"Score\": normalized_scores, \"Sentence\": sentences })\n",
        "df.sort_values(by=['Score'], ascending=False, inplace=True)\n",
        "\n",
        "display(df)"
      ],
      "metadata": {
        "id": "CNLoAh9FWssQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metric 2: Intra-sentence relevancy\n",
        "Score by if current sentence is needded by adjacent sentences."
      ],
      "metadata": {
        "id": "mtycqPqJfA_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Store predictions for each sentence\n",
        "predictions = []\n",
        "\n",
        "# Iterate through sentence pairs\n",
        "for i in range(len(sentences) - 1):\n",
        "    sentence1 = sentences[i]\n",
        "    sentence2 = sentences[i + 1]\n",
        "\n",
        "    # Tokenize and prepare input\n",
        "    inputs = tokenizer(sentence1, sentence2, return_tensors='pt', truncation=True, padding=True, add_special_tokens=True)\n",
        "\n",
        "    # Get model prediction\n",
        "    outputs = model(**inputs)\n",
        "    prediction = torch.argmax(outputs.logits).item()\n",
        "\n",
        "    # Store prediction\n",
        "    predictions.append(prediction)\n",
        "\n",
        "# Handle last sentence (no next sentence)\n",
        "predictions.append(0)  # Assume last sentence doesn't need a next sentence"
      ],
      "metadata": {
        "id": "OGX8E0UOaTjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort_values(by=['Sentence Index'], ascending=True, inplace=True)\n",
        "\n",
        "# Add predictions to DataFrame\n",
        "df = df.assign(**{\"Previous Sentence Needed\": predictions})\n",
        "\n",
        "display(df)"
      ],
      "metadata": {
        "id": "XE9fi5WTbSnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metric 3: Intelligent Sentence-Paragraph Relevancy"
      ],
      "metadata": {
        "id": "KDpPsygPOoZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Time Taken: 13min - 26min"
      ],
      "metadata": {
        "id": "ej1h8aIMQOfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "model = LongformerForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Example usage\n",
        "body_paragraph = paragraph\n",
        "\n",
        "relevance_scores = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    # Prepare the input for Longformer\n",
        "    inputs = tokenizer(\n",
        "        body_paragraph,\n",
        "        sentence,\n",
        "        return_tensors='pt',\n",
        "        max_length=4096,\n",
        "        truncation=True,\n",
        "        padding='max_length'  # Pad to max length to avoid issues with model input size\n",
        "    )\n",
        "\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Assuming binary classification (relevant/not relevant)\n",
        "    relevance_score = torch.softmax(outputs.logits, dim=1)[0][1].item()  # Probability of being relevant\n",
        "    relevance_scores.append((sentence, relevance_score))\n",
        "\n",
        "# Sort sentences based on relevance scores\n",
        "sorted_sentences = sorted(relevance_scores, key=lambda x: x[1], reverse=True)\n",
        "ranked_sentences = [sentence for sentence, score in sorted_sentences]"
      ],
      "metadata": {
        "id": "hOtvxn7cOsNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevance_scores[0]"
      ],
      "metadata": {
        "id": "j0z-kFNeX1WH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_indices = list(range(len(relevance_scores)))\n",
        "scores = [score for sentence, score in relevance_scores]\n",
        "sentences_text = [sentence for sentence, score in relevance_scores]\n",
        "\n",
        "df_relevance = pd.DataFrame({'Sentence Index': sentence_indices, 'Score': scores, 'Sentence': sentences_text})\n",
        "df_relevance"
      ],
      "metadata": {
        "id": "bkdGS6xYYSEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metric 4: Keyword extraction and Ranking\n",
        "using TextRank"
      ],
      "metadata": {
        "id": "BZlo75xBc3b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Add the pytextrank pipeline component to spaCy\n",
        "nlp.add_pipe(\"textrank\")\n",
        "\n",
        "phrase_data = []\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(paragraph)\n",
        "\n",
        "for phrase in doc._.phrases:\n",
        "  phrase_data.append([phrase.text, phrase.rank, phrase.count])\n",
        "\n",
        "df_phrases = pd.DataFrame(phrase_data, columns=['Phrase', 'Rank', 'Count'])\n",
        "df_phrases.sort_values(by=['Rank'], ascending=False, inplace=True)\n",
        "\n",
        "display(df_phrases)"
      ],
      "metadata": {
        "id": "g6sGPwmqbscu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio"
      ],
      "metadata": {
        "id": "aJQglUy2Pz3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metric 5: Silence Detection\n",
        "* From the Paragraph boundaries, get the time in aduio that we care about\n",
        "* For each time in audio we care about, analyze if they are low volume\n",
        "\n",
        "OR\n",
        "* analyze all potential sentence boundaries first\n",
        "* match with end of sentences"
      ],
      "metadata": {
        "id": "LqKPixxmxvqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0: Load audio, extract timestamps\n",
        "\n",
        "SAMPLING_RATE = 16000 # 16 kHz\n",
        "\n",
        "model = load_silero_vad()\n",
        "wav = read_audio(audio_output)\n",
        "speech_timestamps = get_speech_timestamps(wav, model)\n",
        "\n",
        "# Check the shape of the wav tensor\n",
        "print(f\"Audio shape: {wav.shape}\")\n",
        "print(f\"Audio length (seconds): {len(wav) / SAMPLING_RATE:.2f}\")"
      ],
      "metadata": {
        "id": "bEjw2zzdMxz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Speech Intervals\n",
        "speech_intervals = []\n",
        "for i in range(0, len(speech_timestamps)-1):\n",
        "    speech_intervals.append((speech_timestamps[i]['start'] / SAMPLING_RATE, speech_timestamps[i+1]['end'] / SAMPLING_RATE))\n",
        "\n",
        "# Silence Intervals\n",
        "silence_intervals = []\n",
        "for i in range(1, len(speech_timestamps)):\n",
        "    silence_start = speech_timestamps[i-1]['end']  # End of previous speech segment\n",
        "    silence_end = speech_timestamps[i]['start']     # Start of current speech segment\n",
        "    silence_intervals.append((silence_start / SAMPLING_RATE, silence_end / SAMPLING_RATE))"
      ],
      "metadata": {
        "id": "aXomQWsy4_0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(speech_timestamps[0:3])\n",
        "print(speech_intervals[0:3])\n",
        "print(silence_intervals[0:3])"
      ],
      "metadata": {
        "id": "8ela8OxwFsjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhancing SRV\n",
        "# import re\n",
        "\n",
        "# # Split the enhanced text by sentence boundaries\n",
        "# sentences = re.split(r'(?<=[.!?]) +', enhanced_text)\n",
        "\n",
        "# # Update subtitles with enhanced sentences\n",
        "# for i, subtitle in enumerate(subtitles):\n",
        "#     subtitle.content = sentences[i] if i < len(sentences) else \"\"\n",
        "\n",
        "# # Reconstruct the SRT\n",
        "# enhanced_srt_content = srt.compose(subtitles)\n",
        "\n",
        "# # Save the improved SRT\n",
        "# with open(\"enhanced_subtitle_file.srt\", \"w\") as f:\n",
        "#     f.write(enhanced_srt_content)\n"
      ],
      "metadata": {
        "id": "85Fu7ndmxrnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video"
      ],
      "metadata": {
        "id": "rJ-tgGS8Sr5P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_1NCpG3SStHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PostProcessing"
      ],
      "metadata": {
        "id": "xlckTwsoxVqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the audio file\n",
        "audio = AudioSegment.from_file(\"input_audio.wav\")\n",
        "\n",
        "# Define segments\n",
        "first_part = audio[:180000]  # Up to 3:00 (in milliseconds)\n",
        "second_part = audio[186000:]  # From 3:06 onward\n",
        "\n",
        "# Combine segments\n",
        "edited_audio = first_part + second_part\n",
        "\n",
        "# Export the edited audio\n",
        "edited_audio.export(\"output_audio.wav\", format=\"wav\")"
      ],
      "metadata": {
        "id": "Qgua7QztxZqY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}